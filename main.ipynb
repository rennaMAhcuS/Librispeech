{
 "cells": [
  {
   "cell_type": "code",
   "id": "07d3db9e",
   "metadata": {},
   "source": [
    "\n",
    "# Cell: Import Libraries\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "from glob import glob\n",
    "import python_speech_features as psf\n",
    "from scipy.io.wavfile import read\n",
    "from IPython.display import Audio\n",
    "import json\n",
    "import g2p_en\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import shutil\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9f1e028",
   "metadata": {},
   "source": [
    "# Specify the download directory\n",
    "download_dir = \"C:/Users/abhin/nltk_data\"\n",
    "\n",
    "# Download both resources to the specified directory\n",
    "nltk.download('punkt', download_dir=download_dir)\n",
    "nltk.download('averaged_perceptron_tagger_eng', download_dir=download_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54ef27cd",
   "metadata": {},
   "source": [
    "# URL for the dataset\n",
    "url = \"http://www.openslr.org/resources/12/train-clean-100.tar.gz\" \n",
    "\n",
    "# Destination path for downloading\n",
    "download_path = \"train-clean-100.tar.gz\"\n",
    "\n",
    "# Function to download a file with progress bar\n",
    "def download_file(url, destination_path):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total_size = int(r.headers.get('content-length', 0))\n",
    "        with open(destination_path, 'wb') as f:\n",
    "            for chunk in tqdm(r.iter_content(chunk_size=1024), total=total_size // 1024, unit='KB', desc=\"Downloading\"):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "# # Download the file\n",
    "# download_file(url, download_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e8c0fe9d",
   "metadata": {},
   "source": [
    "# Function to extract tar.gz file\n",
    "def extract_file(tar_path, dest_dir):\n",
    "    try:\n",
    "        with tarfile.open(tar_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=dest_dir)\n",
    "        print(f\"Dataset extracted to {dest_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")\n",
    "\n",
    "# Download the file\n",
    "if not os.path.exists(download_path):\n",
    "    download_file(url, download_path)\n",
    "\n",
    "# Define the destination extraction folder in the current working directory\n",
    "extracted_dir = os.path.join(os.getcwd(), \"audio_files\")\n",
    "\n",
    "# Extract the tar.gz file if not already extracted\n",
    "if os.path.exists(download_path) and not os.path.exists(extracted_dir):\n",
    "    extract_file(download_path, extracted_dir)\n",
    "else:\n",
    "    if os.path.exists(extracted_dir):\n",
    "        print(f\"Dataset already extracted at {extracted_dir}\")\n",
    "    else:\n",
    "        print(f\"Download path does not exist: {download_path}\")\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c710b9d3",
   "metadata": {},
   "source": [
    "# Cell: Locate all .flac files\n",
    "flac_files = glob(os.path.join(extracted_dir, \"**\", \"*.flac\"), recursive=True)\n",
    "print(f\"Found {len(flac_files)} .flac files\")\n",
    "print(\"Example file:\", flac_files[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1fc20065",
   "metadata": {},
   "source": [
    "# Path settings\n",
    "source_root = \"audio_files/LibriSpeech/train-clean-100\"\n",
    "wav_root = \"train-clean-100-wav\"\n",
    "\n",
    "# Find all .flac files under the source directory\n",
    "flac_files = []\n",
    "for root, _, files in os.walk(source_root):\n",
    "    for file in files:\n",
    "        if file.endswith(\".flac\"):\n",
    "            flac_files.append(os.path.join(root, file))\n",
    "\n",
    "# Convert and save .wav files under simplified structure\n",
    "def convert_flac_to_wav(flac_files, source_root, wav_root):\n",
    "    for flac_path in tqdm(flac_files, desc=\"Converting to WAV\"):\n",
    "        # Get relative path from source_root (e.g., 19/198/19-198-0000.flac)\n",
    "        relative_path = os.path.relpath(flac_path, source_root)\n",
    "        wav_path = os.path.join(wav_root, os.path.splitext(relative_path)[0] + \".wav\")\n",
    "        \n",
    "        # Create directory if needed\n",
    "        os.makedirs(os.path.dirname(wav_path), exist_ok=True)\n",
    "        \n",
    "        # Load and write audio\n",
    "        audio, sr = sf.read(flac_path)\n",
    "        sf.write(wav_path, audio, sr)\n",
    "        print(f\"Saved: {wav_path}\")\n",
    "\n",
    "# convert_flac_to_wav(flac_files, source_root, wav_root)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5067e72b",
   "metadata": {},
   "source": [
    "# Function to copy transcripts to the new structure\n",
    "def copy_transcripts(source_root, wav_root):\n",
    "    for root, _, files in tqdm(os.walk(source_root), desc=\"Copying Transcripts\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".trans.txt\"):\n",
    "                # Full path to the transcript file\n",
    "                transcript_file = os.path.join(root, file)\n",
    "                \n",
    "                # Get the relative path from source_root (e.g., 19/198/19-198.trans.txt)\n",
    "                relative_path = os.path.relpath(root, source_root)\n",
    "                \n",
    "                # Create the corresponding directory in the new location\n",
    "                dest_dir = os.path.join(wav_root, relative_path)\n",
    "                os.makedirs(dest_dir, exist_ok=True)\n",
    "                \n",
    "                # Construct the destination path for the transcript file\n",
    "                transcript_dest = os.path.join(dest_dir, file)\n",
    "                \n",
    "                # Copy the transcript file\n",
    "                shutil.copy(transcript_file, transcript_dest)\n",
    "                print(f\"Copied: {transcript_dest}\")\n",
    "\n",
    "# Call the function to copy transcripts\n",
    "# copy_transcripts(source_root, wav_root)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a15b74c0",
   "metadata": {},
   "source": [
    "import glob\n",
    "wav_files = glob.glob(\"train-clean-100-wav/**/*.wav\", recursive=True)\n",
    "print(f\"Found {len(wav_files)} .wav files\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9abb1fef",
   "metadata": {},
   "source": [
    "# Cell: Extract MFCC features\n",
    "\n",
    "def extract_mfcc(wav_path, num_mfcc=13):\n",
    "    sr, signal = read(wav_path)  # Correct order: sr, signal from scipy read\n",
    "    mfcc_feat = psf.mfcc(signal, sr, numcep=num_mfcc)\n",
    "    return mfcc_feat\n",
    "\n",
    "# Example on one file\n",
    "sample_mfcc = extract_mfcc(wav_files[0])\n",
    "print(\"MFCC shape:\", sample_mfcc.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "207c31cb",
   "metadata": {},
   "source": [
    "# Cell: Visualize waveform and MFCCs\n",
    "def visualize_waveform_and_mfcc(wav_path):\n",
    "    signal, sr = librosa.load(wav_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(signal)\n",
    "    plt.title(\"Waveform\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    librosa.display.specshow(mfcc.T, sr=sr, x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title(\"MFCC\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test on one file\n",
    "visualize_waveform_and_mfcc(wav_files[0])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15ae171f",
   "metadata": {},
   "source": [
    "\n",
    "# Limit to ~1 hour of data (assuming ~6 minutes per file on average)\n",
    "target_duration = 60 * 60  # 1 hour in seconds\n",
    "selected_files = []\n",
    "\n",
    "current_duration = 0\n",
    "i = 0\n",
    "while current_duration < target_duration:\n",
    "    file = wav_files[i]  # Randomly select a file\n",
    "    signal, sr = librosa.load(file, sr=None)  # Load the file\n",
    "    current_duration += len(signal) / sr\n",
    "    selected_files.append(file)\n",
    "    i+=1\n",
    "\n",
    "# Save the selected files list\n",
    "with open(\"selected_files.json\", \"w\") as f:\n",
    "    json.dump(selected_files, f)\n",
    "\n",
    "print(f\"Selected {len(selected_files)} files, total duration: {current_duration / 60:.2f} minutes\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5672813",
   "metadata": {},
   "source": [
    "# Load the selected files\n",
    "with open(\"selected_files.json\", \"r\") as f:\n",
    "    selected_files = json.load(f)\n",
    "\n",
    "# Extract MFCCs\n",
    "mfcc_data = []  # list of dicts: {\"path\": ..., \"mfcc\": ...}\n",
    "\n",
    "for wav_path in tqdm(selected_files, desc=\"Extracting MFCCs\"):\n",
    "    try:\n",
    "        # Load the signal using soundfile (sf)\n",
    "        signal, sr = sf.read(wav_path)\n",
    "        \n",
    "        # Check if signal is a numpy array (1D for mono audio)\n",
    "        if not isinstance(signal, np.ndarray) or len(signal.shape) != 1:\n",
    "            print(f\"Warning: {wav_path} has an invalid signal format\")\n",
    "            continue\n",
    "        \n",
    "        # Compute MFCC features using python_speech_features\n",
    "        mfcc_feat = psf.mfcc(signal, sr, numcep=13)\n",
    "\n",
    "        # Check if mfcc_feat is a 2D numpy array (valid MFCC features)\n",
    "        if not isinstance(mfcc_feat, np.ndarray) or len(mfcc_feat.shape) != 2:\n",
    "            print(f\"Warning: {wav_path} returned invalid MFCC features\")\n",
    "            continue\n",
    "        \n",
    "        # Append the MFCC data to the list\n",
    "        mfcc_data.append({\n",
    "            \"path\": wav_path,\n",
    "            \"mfcc\": mfcc_feat.tolist()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {wav_path}: {e}\")\n",
    "\n",
    "# Save MFCC data to a file\n",
    "with open(\"mfcc_features_subset.json\", \"w\") as f:\n",
    "    json.dump(mfcc_data, f)\n",
    "\n",
    "print(\"Saved MFCC features to mfcc_features_subset.json\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5e3788e",
   "metadata": {},
   "source": [
    "from g2p_en import G2p\n",
    "\n",
    "g2p = G2p()\n",
    "\n",
    "# Example word to phoneme conversion\n",
    "word = \"hello\"\n",
    "phonemes = g2p(word)\n",
    "print(f\"Word: {word} -> Phonemes: {phonemes}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85e19bc0",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def clean_transcript(transcript):\n",
    "    # Lowercase and remove unwanted characters (punctuation, special symbols)\n",
    "    transcript = transcript.lower()  # Make lowercase\n",
    "    transcript = re.sub(r'[^a-z\\s]', '', transcript)  # Remove non-alphabetical characters\n",
    "    return transcript\n",
    "\n",
    "# Example\n",
    "transcript = \"Hello, world! How are you?\"\n",
    "cleaned_transcript = clean_transcript(transcript)\n",
    "print(f\"Cleaned Transcript: {cleaned_transcript}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a47b89d5",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Function to get the transcript for a given wav file\n",
    "def get_transcript_for_wav(wav_path):\n",
    "    # Extract the speaker and chapter info from the path\n",
    "    speaker_id = wav_path.split(\"\\\\\")[1]  # Example: '19'\n",
    "    chapter_id = wav_path.split(\"\\\\\")[2]  # Example: '198'\n",
    "    audio_id = wav_path.split(\"\\\\\")[-1].replace(\".wav\", \"\")  # Example: '19-198-0001'\n",
    "\n",
    "    # Construct the transcript filename based on speaker_id and chapter_id\n",
    "    transcript_file = os.path.join(\"train-clean-100-wav\", speaker_id, chapter_id, f\"{speaker_id}-{chapter_id}.trans.txt\")\n",
    "\n",
    "    # Read the transcript file and find the corresponding line for the audio file\n",
    "    try:\n",
    "        with open(transcript_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Find the line corresponding to the audio_id (audio file name)\n",
    "        for line in lines:\n",
    "            if audio_id in line:\n",
    "                # Split the line to extract the actual transcript (skip the first column)\n",
    "                transcript = line.split(\" \", 1)[1].strip()\n",
    "                return transcript\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Transcript file {transcript_file} not found.\")\n",
    "        return None  # If the transcript file is not found, return None\n",
    "\n",
    "    return None  # Return None if no matching transcript is found\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9f93ee5",
   "metadata": {},
   "source": [
    "# Initialize G2p model\n",
    "g2p = G2p()\n",
    "\n",
    "# Step 1: Clean the transcript by removing unnecessary characters and normalizing the text\n",
    "def clean_transcript(transcript):\n",
    "    # Convert to lowercase and remove punctuation (except spaces)\n",
    "    transcript = transcript.lower()\n",
    "    transcript = re.sub(r\"[^a-zA-Z\\s]\", \"\", transcript)\n",
    "    return transcript\n",
    "\n",
    "# Step 2: Convert a cleaned transcript to phonemes\n",
    "def convert_to_phonemes(cleaned_transcript):\n",
    "    words = cleaned_transcript.split()\n",
    "    phonemes = []\n",
    "\n",
    "    for word in words:\n",
    "        phonemes.extend(g2p(word))  # Get phoneme representation for each word\n",
    "\n",
    "    return phonemes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8693bd92",
   "metadata": {},
   "source": [
    "# Load the MFCC data\n",
    "with open(\"mfcc_features_subset.json\", \"r\") as f:\n",
    "    mfcc_data = json.load(f)\n",
    "\n",
    "# Create a list to hold MFCC data with phoneme mappings\n",
    "mfcc_with_phonemes = []\n",
    "\n",
    "# Map MFCC features to phonemes\n",
    "for data in tqdm(mfcc_data, desc=\"Mapping MFCCs to Phonemes\"):\n",
    "    wav_path = data[\"path\"]\n",
    "    mfcc_feat = data[\"mfcc\"]\n",
    "    \n",
    "    # Retrieve the corresponding transcript for the audio file\n",
    "    transcript = get_transcript_for_wav(wav_path)\n",
    "    if transcript is None:\n",
    "        continue  # Skip if no transcript is found\n",
    "    \n",
    "    # Clean the transcript\n",
    "    cleaned_transcript = clean_transcript(transcript)\n",
    "    \n",
    "    # Convert the cleaned transcript to phonemes\n",
    "    phonemes = convert_to_phonemes(cleaned_transcript)\n",
    "    \n",
    "    # Append the MFCC features and the phoneme sequence\n",
    "    mfcc_with_phonemes.append({\n",
    "        \"path\": wav_path,\n",
    "        \"mfcc\": mfcc_feat,\n",
    "        \"phonemes\": phonemes\n",
    "    })\n",
    "\n",
    "# Save the new data (MFCC + phonemes)\n",
    "with open(\"mfcc_with_phonemes.json\", \"w\") as f:\n",
    "    json.dump(mfcc_with_phonemes, f)\n",
    "\n",
    "print(\"Mapped MFCCs to Phonemes and saved.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e67d761",
   "metadata": {},
   "source": [
    "# Create a phoneme to index mapping (dictionary)\n",
    "phoneme_set = set()  # To store unique phonemes\n",
    "\n",
    "# Collect all unique phonemes from the mapped data\n",
    "for data in mfcc_with_phonemes:\n",
    "    phonemes = data[\"phonemes\"]\n",
    "    phoneme_set.update(phonemes)\n",
    "\n",
    "# Create the phoneme-to-index mapping\n",
    "phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(sorted(phoneme_set))}\n",
    "\n",
    "# Save the phoneme-to-index mapping for later use\n",
    "with open(\"phoneme_to_index.json\", \"w\") as f:\n",
    "    json.dump(phoneme_to_index, f)\n",
    "\n",
    "print(\"Phoneme-to-index mapping created and saved.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36dc5189",
   "metadata": {},
   "source": [
    "# Convert phoneme sequences to their corresponding indices\n",
    "for data in tqdm(mfcc_with_phonemes, desc=\"Converting Phonemes to Indices\"):\n",
    "    phonemes = data[\"phonemes\"]\n",
    "    \n",
    "    # Convert phonemes to indices using the phoneme-to-index mapping\n",
    "    phoneme_indices = []\n",
    "    for phoneme in phonemes:\n",
    "        if phoneme in phoneme_to_index:\n",
    "            phoneme_indices.append(phoneme_to_index[phoneme])\n",
    "        else:\n",
    "            phoneme_indices.append(phoneme_to_index.get(\"<UNK>\", -1))  # Use -1 or <UNK> for missing phonemes\n",
    "    \n",
    "    # Replace the phonemes with their corresponding indices\n",
    "    data[\"phoneme_indices\"] = phoneme_indices\n",
    "\n",
    "# Save the new data (MFCC + phoneme indices)\n",
    "with open(\"mfcc_with_phoneme_indices.json\", \"w\") as f:\n",
    "    json.dump(mfcc_with_phonemes, f)\n",
    "\n",
    "print(\"Phonemes converted to indices and saved.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0979a09d",
   "metadata": {},
   "source": [
    "# Prepare features (MFCCs) and labels (phoneme indices)\n",
    "X = []  # Features: list of MFCC sequences (each a 2D array)\n",
    "lengths = []  # Needed for hmmlearn to know sequence boundaries\n",
    "\n",
    "# Check if `mfcc_with_phonemes` is populated\n",
    "print(f\"mfcc_with_phonemes contains {len(mfcc_with_phonemes)} elements\")\n",
    "\n",
    "# Iterate over each data point and collect MFCCs and lengths\n",
    "for data in mfcc_with_phonemes:\n",
    "    mfcc = np.array(data[\"mfcc\"])\n",
    "    X.append(mfcc)\n",
    "    lengths.append(len(mfcc))\n",
    "\n",
    "# Concatenate all MFCCs into a single 2D array\n",
    "X_concat = np.concatenate(X, axis=0)\n",
    "\n",
    "# Save the prepared features and sequence lengths\n",
    "np.save(\"mfcc_features_concat.npy\", X_concat)\n",
    "np.save(\"mfcc_lengths.npy\", lengths)\n",
    "\n",
    "print(\"Features and sequence lengths prepared for HMM training and saved.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e9f6287",
   "metadata": {},
   "source": [
    "# Load phoneme-to-index mapping and training data\n",
    "with open(\"phoneme_to_index.json\", \"r\") as f:\n",
    "    phoneme_to_index = json.load(f)\n",
    "\n",
    "index_to_phoneme = {v: k for k, v in phoneme_to_index.items()}\n",
    "\n",
    "# Load MFCC data with phoneme indices\n",
    "with open(\"mfcc_with_phoneme_indices.json\", \"r\") as f:\n",
    "    mfcc_data = json.load(f)\n",
    "\n",
    "# Organize MFCC sequences per phoneme\n",
    "phoneme_sequences = defaultdict(list)\n",
    "\n",
    "for sample in mfcc_data:\n",
    "    mfcc = np.array(sample[\"mfcc\"])\n",
    "    labels = sample[\"phoneme_indices\"]\n",
    "\n",
    "    if len(labels) == 0 or len(mfcc) < len(labels):\n",
    "        continue\n",
    "\n",
    "    # Split MFCCs equally among phonemes (approximate)\n",
    "    chunk_size = len(mfcc) // len(labels)\n",
    "    for i, phoneme in enumerate(labels):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size\n",
    "        phoneme_sequences[phoneme].append(mfcc[start:end])\n",
    "\n",
    "# Train one HMM per phoneme\n",
    "phoneme_models = {}\n",
    "n_components = 3  # Number of hidden states per phoneme model\n",
    "\n",
    "for phoneme_idx, sequences in tqdm(phoneme_sequences.items(), desc=\"Training HMMs\"):\n",
    "    try:\n",
    "        # Concatenate all MFCCs for the phoneme\n",
    "        X = np.concatenate(sequences, axis=0)\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "        # Initialize and train the HMM\n",
    "        model = hmm.GaussianHMM(n_components=n_components, covariance_type=\"diag\", n_iter=100)\n",
    "        model.fit(X, lengths)\n",
    "\n",
    "        # Apply Laplace smoothing if necessary\n",
    "        laplace_smoothing_constant = 1e-5\n",
    "        zero_rows = np.all(model.transmat_ == 0, axis=1)\n",
    "        if np.any(zero_rows):\n",
    "            print(f\"Applying Laplace smoothing to phoneme {index_to_phoneme[phoneme_idx]}\")\n",
    "            model.transmat_[zero_rows] += laplace_smoothing_constant\n",
    "        \n",
    "        # Normalize the transition matrix rows to ensure they sum to 1\n",
    "        model.transmat_ /= model.transmat_.sum(axis=1, keepdims=True)\n",
    "\n",
    "        phoneme_models[phoneme_idx] = model\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to train HMM for phoneme {index_to_phoneme[phoneme_idx]}: {e}\")\n",
    "\n",
    "# Save all phoneme models to a file\n",
    "with open(\"hmm_phoneme_models.pkl\", \"wb\") as f:\n",
    "    pickle.dump(phoneme_models, f)\n",
    "\n",
    "print(\"All phoneme HMMs trained, smoothed (if needed), and saved.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98a6c114",
   "metadata": {},
   "source": [
    "# Load test data (MFCCs with phoneme indices)\n",
    "with open(\"mfcc_with_phoneme_indices.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Load trained phoneme models\n",
    "with open(\"hmm_phoneme_models.pkl\", \"rb\") as f:\n",
    "    phoneme_models = pickle.load(f)\n",
    "\n",
    "# Reverse mapping from index to phoneme\n",
    "with open(\"phoneme_to_index.json\", \"r\") as f:\n",
    "    phoneme_to_index = json.load(f)\n",
    "\n",
    "\n",
    "# Function to decode using HMMs with Viterbi algorithm\n",
    "def decode_with_hmms_viterbi(mfcc_sequence, models):\n",
    "    predicted = []\n",
    "    for frame in mfcc_sequence:\n",
    "        frame = frame.reshape(1, -1)  # Ensure correct shape for the model\n",
    "        scores = {phoneme_idx: model.score(frame) for phoneme_idx, model in models.items()}\n",
    "        best_phoneme = max(scores, key=scores.get)\n",
    "        predicted.append(best_phoneme)\n",
    "    return predicted\n",
    "\n",
    "# Function to convert indices to phonemes\n",
    "def indices_to_phonemes(indices):\n",
    "    return [index_to_phoneme[i] for i in indices]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8cbdc083",
   "metadata": {},
   "source": [
    "# Load test data (MFCCs with phoneme indices)\n",
    "with open(\"mfcc_with_phoneme_indices.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Collect predicted and actual phoneme sequences\n",
    "predicted_phonemes = []\n",
    "actual_phonemes = []\n",
    "\n",
    "for i, data in enumerate(tqdm(test_data, desc=\"Testing Model\")):\n",
    "    mfcc = np.array(data[\"mfcc\"])  # MFCC sequence\n",
    "    actual_sequence = data[\"phoneme_indices\"]  # Actual phoneme indices\n",
    "\n",
    "    # Decode using Viterbi\n",
    "    predicted_sequence = decode_with_hmms_viterbi(mfcc, phoneme_models)\n",
    "    \n",
    "    print(f\"Actual:    {actual_sequence[:10]}\")\n",
    "    print(f\"Predicted: {predicted_sequence[:10]}\")\n",
    "\n",
    "    # --- Per-sample accuracy logging ---\n",
    "    min_len = min(len(actual_sequence), len(predicted_sequence))\n",
    "    trimmed_actual = actual_sequence[:min_len]\n",
    "    trimmed_predicted = predicted_sequence[:min_len]\n",
    "    match_count = sum(a == b for a, b in zip(trimmed_actual, trimmed_predicted))\n",
    "    sample_accuracy = match_count / min_len if min_len > 0 else 0\n",
    "    print(f\"Sample {i+1}: Accuracy = {sample_accuracy * 100:.2f}%\")\n",
    "    # -----------------------------------\n",
    "\n",
    "    # Convert indices to phonemes\n",
    "    predicted_phonemes.append(indices_to_phonemes(predicted_sequence))\n",
    "    actual_phonemes.append(indices_to_phonemes(actual_sequence))\n",
    "\n",
    "# Calculate overall accuracy\n",
    "flat_predicted = [phoneme for sublist in predicted_phonemes for phoneme in sublist]\n",
    "flat_actual = [phoneme for sublist in actual_phonemes for phoneme in sublist]\n",
    "\n",
    "accuracy = accuracy_score(flat_actual, flat_predicted)\n",
    "print(f\"\\nOverall Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Show a few example predictions\n",
    "for i in range(min(5, len(predicted_phonemes))):\n",
    "    print(f\"\\nTest Sample {i + 1}:\")\n",
    "    print(f\"Predicted: {predicted_phonemes[i]}\")\n",
    "    print(f\"Actual:    {actual_phonemes[i]}\")\n",
    "    print(\"-\" * 50)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
